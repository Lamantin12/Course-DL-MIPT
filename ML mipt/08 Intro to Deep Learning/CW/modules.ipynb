{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b67c8f4",
   "metadata": {},
   "source": [
    "# 1 Basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c971ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box)\n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`:\n",
    "        output = module.forward(input)\n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule.\n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.update_output(inp)\n",
    "\n",
    "    def backward(self, inp, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        This includes\n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.update_grad_input(inp, grad_output)\n",
    "        self.acc_grad_params(inp, grad_output)\n",
    "        return self.grad_input\n",
    "\n",
    "    def update_output(self, inp):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        Make sure to both store the data in `output` field and return it.\n",
    "        \"\"\"\n",
    "        # The easiest case:\n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "        pass\n",
    "\n",
    "    def update_grad_input(self, inp, grad_output):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input.\n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        # The easiest case:\n",
    "        # self.gradInput = gradOutput\n",
    "        # return self.gradInput\n",
    "        pass\n",
    "    \n",
    "    def acc_grad_parameters(self, inp, grad_output):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def zero_grad_parameters(self):\n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def get_grad_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Module\"\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially.\n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "\n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def update_output(self, inp):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "        Just write a little loop.\n",
    "        \"\"\"\n",
    "        self.output = inp\n",
    "        for module in self.modules:\n",
    "            self.output = module.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, inp, grad_output):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)\n",
    "            gradInput = module[0].backward(input, g_1)\n",
    "        !!!\n",
    "        To ech module you need to provide the input, module saw while forward pass,\n",
    "        it is used while computing gradients.\n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
    "        and NOT `input` to this Sequential module.\n",
    "        !!!\n",
    "        \"\"\"\n",
    "        for i in range(len(self.modules) - 1, 0, -1):\n",
    "            grad_output = self.modules[i].backward(self.modules[i - 1].output, grad_output)\n",
    "        self.grad_input = self.modules[0].backward(inp, grad_output)\n",
    "        return self.grad_input\n",
    "\n",
    "    def zero_grad_parameters(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad_parameters\n",
    "    \n",
    "    def get_grad_parameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.get_grad_parameters() for x in self.modules]\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()\n",
    "\n",
    "class Criterion(object):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function\n",
    "            associated to the criterion and return the result.\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.update_output(inp, target)\n",
    "\n",
    "    def backward(self, inp, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result.\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.update_grad_input(inp, target)\n",
    "\n",
    "    def update_output(self, inp, output):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fce50",
   "metadata": {},
   "source": [
    "# Layers\n",
    "## 1. Linear transform layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10dc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        stdv = 1/np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = (n_out))\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def update_output(self, inp):\n",
    "        self.output = inp@self.W.T + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, inp, grad_output):\n",
    "        self.grad_input = grad_output@self.W\n",
    "        return self.grad_input\n",
    "    \n",
    "    def acc_grad_parameters(self, inp, grad_output):\n",
    "        self.gradW = np.sum(inp[:, None, :] * grad_output[:, :, None], axis=0)\n",
    "        self.gradb = np.sum(grad_output, axis=0)\n",
    "        \n",
    "    def zero_grad_parameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def get_grad_parameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear {self.W.shape[0]} -> {self.W.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2927f",
   "metadata": {},
   "source": [
    "# Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e86bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    \n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "        \n",
    "    def update_output(self, inp):\n",
    "        self.output = inp.copy()\n",
    "        mask = inp < 0\n",
    "        self.output[mask] *= self.slope\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, inp, grad_output):\n",
    "        self.grad_input = grad_output.copy()\n",
    "        mask = inp < 0\n",
    "        self.grad_input[mask] *= self.slope\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcb0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
